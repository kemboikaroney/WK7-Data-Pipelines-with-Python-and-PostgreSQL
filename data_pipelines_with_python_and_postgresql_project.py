# -*- coding: utf-8 -*-
"""Data Pipelines with Python and PostgreSQL Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BWNMQbRXKEcrMZv5jl3K7pHyqqUhjkCd
"""

# Import the necessary libraries 
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from sklearn.preprocessing import StandardScaler

# Define the SQL database connection for  Google Cloud 
POSTGRES_ADDRESS = '34.136.94.60'
POSTGRES_PORT = '5432'
POSTGRES_USERNAME = 'admin'
POSTGRES_PASSWORD = 'password'
POSTGRES_DBNAME = 'telecom'

# Define the database engine
postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'
                .format(username=POSTGRES_USERNAME,
                        password=POSTGRES_PASSWORD,
                        ipaddress=POSTGRES_ADDRESS,
                        port=POSTGRES_PORT,
                        dbname=POSTGRES_DBNAME))
engine = create_engine(postgres_str)

def extract_data():
    # Load the equipment sensor data
    equipment_sensor_df = pd.read_csv('equipment_sensor.csv')

    # Load the network sensor data
    network_data_df = pd.read_csv('network_sensor.csv')

    # Load the maintenance records data
    maintenance_records_df = pd.read_csv('maintenance_records.csv')

    return equipment_sensor_df, network_data_df, maintenance_records_df

def transform_data(equipment_sensor, network_data, maintenance_records):
    # Remove duplicates
    equipment_sensor.drop_duplicates(inplace=True)
    network_data.drop_duplicates(inplace=True)
    maintenance_records.drop_duplicates(inplace=True)

    # Fix missing data
    equipment_sensor.fillna(method='ffill', inplace=True)
    network_data.fillna(method='ffill', inplace=True)
    maintenance_records.fillna(method='ffill', inplace=True)

    # Normalize data 
    # using the StandardScaler to normalize the sensor_reading column in both equipment_sensor and network_data
    scaler = StandardScaler()
    equipment_sensor['sensor_reading'] = scaler.fit_transform(equipment_sensor[['sensor_reading']])
    network_data['sensor_reading'] = scaler.fit_transform(network_data[['sensor_reading']])

    # Aggregate the data
    equipment_summary = equipment_sensor.groupby('ID').agg({'date': ['min', 'max'], 'sensor_reading': ['mean', 'max']})
    equipment_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']
    network_summary = network_data.groupby('ID').agg({'date': ['min', 'max'], 'sensor_reading': ['mean', 'max']})
    network_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']

    # Join the data
    sensor_summary = pd.merge(equipment_summary, network_summary, how='outer', left_index=True, right_index=True)
    sensor_summary = sensor_summary.reset_index()
    sensor_summary = sensor_summary.rename(columns={'ID': 'equipment_ID'})

    maintenance_records = maintenance_records[['date', 'time', 'equipment_ID', 'maintenance_type']]
    maintenance_records['date_time'] = pd.to_datetime(maintenance_records['date'] + ' ' + maintenance_records['time'])
    maintenance_records.drop(['date', 'time'], axis=1, inplace=True)

    return sensor_summary, maintenance_records

def load_data(sensor_summary, maintenance_records):
    # Load the data to PostgreSQL
    sensor_summary.to_sql('sensor_summary', engine, if_exists='replace')
    maintenance_records.to_sql('maintenance_records', engine, if_exists='replace')


if __name__ == '__main__':
    equipment_sensor, network_data, maintenance_records = extract_data()
    sensor_summary, maintenance_records = transform_data(equipment_sensor, network_data, maintenance_records)
    load_data(sensor_summary, maintenance_records)